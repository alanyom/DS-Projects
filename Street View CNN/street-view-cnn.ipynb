{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2625728,"sourceType":"datasetVersion","datasetId":1596369}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np  # For numerical computations\nimport pandas as pd  # For handling datasets (e.g., CSVs)\nimport tensorflow as tf  # Core deep learning framework\nfrom matplotlib import pyplot as plt  # For plotting and visualization\nfrom tensorflow import keras  # High-level API for neural networks\nfrom tensorflow.keras.models import Sequential, load_model  # For building sequential models\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization  # Core CNN layers\nfrom tensorflow.keras.optimizers import Adam  # Optimizer (if needed separately)\nfrom tensorflow.keras.utils import to_categorical  # For one-hot encoding labels\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom keras_tuner import HyperModel\nfrom keras_tuner.tuners import BayesianOptimization\nimport keras_tuner as kt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n\nX_train = np.load('/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/X_train.npy')\nY_train = np.load('/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/y_train.npy')\n\n# Load test dataset\nX_test = np.load('/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/X_test.npy')\nY_test = np.load('/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/y_test.npy')\n\n#X_train = X_train / 255.0\n#X_test = X_test / 255.0\nX_train = X_train.transpose(3, 0, 1, 2)\nX_test = X_test.transpose(3, 0, 1, 2)\nX_train_resized = tf.image.resize(X_train, (32, 32))\nX_test_resized = tf.image.resize(X_test, (32, 32))\n\nY_train_onehot = tf.keras.utils.to_categorical(Y_train, num_classes=10)\nY_test_onehot = tf.keras.utils.to_categorical(Y_test, num_classes=10)\n\n\ndef build_model(hp):\n    model = Sequential()\n    \n    model.add(Conv2D(\n        filters=hp.Int('filters', min_value=32, max_value=128, step=32),\n        kernel_size=(3, 3),\n        activation='relu',\n        input_shape=(32, 32, 3)\n    ))\n    model.add(MaxPooling2D(2, 2))\n    model.add(Flatten())\n    \n    model.add(Dense(\n        hp.Int('units', min_value=64, max_value=512, step=64),\n        activation='relu'\n    ))\n    \n    model.add(Dense(10, activation='softmax'))\n    \n    model.compile(\n        optimizer=hp.Choice('optimizer', values=['adam', 'sgd']),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n\n\ntuner = kt.BayesianOptimization(\n    build_model,\n    objective='val_accuracy',  # Maximize validation accuracy\n    max_trials=5,  # Number of different hyperparameter configurations to test\n    executions_per_trial=1,  # Number of executions for each configuration\n    directory='my_dir',  # Directory where results will be saved\n    project_name='hyperparameter_tuning'\n)\n\n\n\ntuner.search(X_train_resized, Y_train_onehot, epochs=10, validation_data=(X_test_resized, Y_test_onehot))\n\n# Get the best hyperparameters\nbest_hp = tuner.get_best_hyperparameters()[0]\nprint(\"Best hyperparameters:\", best_hp)\n\n# You can now build the best model and train it\nbest_model = tuner.hypermodel.build(best_hp)\n\n# Train the model\nbest_model.fit(X_train_resized, Y_train_onehot, epochs=10, validation_data=(X_test_resized, Y_test_onehot))\n\n# Save the best model\nbest_model.save('best_model.h5')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T02:57:26.106354Z","iopub.execute_input":"2025-01-17T02:57:26.106705Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/y_train.npy\n/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/y_test.npy\n/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/X_test.npy\n/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/X_train.npy\nReloading Tuner from my_dir/hyperparameter_tuning/tuner0.json\n\nSearch: Running Trial #2\n\nValue             |Best Value So Far |Hyperparameter\n96                |32                |filters\n512               |256               |units\nadam              |sgd               |optimizer\n\nEpoch 1/10\n\u001b[1m2290/2290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 176ms/step - accuracy: 0.5353 - loss: 1.4306 - val_accuracy: 0.8007 - val_loss: 0.7177\nEpoch 2/10\n\u001b[1m2290/2290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 176ms/step - accuracy: 0.8420 - loss: 0.5566 - val_accuracy: 0.8311 - val_loss: 0.5996\nEpoch 3/10\n\u001b[1m2290/2290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 175ms/step - accuracy: 0.8696 - loss: 0.4482 - val_accuracy: 0.8452 - val_loss: 0.5684\nEpoch 4/10\n\u001b[1m2290/2290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 176ms/step - accuracy: 0.8886 - loss: 0.3787 - val_accuracy: 0.8457 - val_loss: 0.5657\nEpoch 5/10\n\u001b[1m1142/2290\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3:14\u001b[0m 169ms/step - accuracy: 0.9045 - loss: 0.3257","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\nX_train = np.load('/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/X_train.npy')\nY_train = np.load('/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/y_train.npy')\n\n# Load test dataset\nX_test = np.load('/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/X_test.npy')\nY_test = np.load('/kaggle/input/street-view-house-numbers-svhn-dataset-numpy/y_test.npy')\n\n#X_train = X_train / 255.0\n#X_test = X_test / 255.0\nX_train = X_train.transpose(3, 0, 1, 2)\nX_test = X_test.transpose(3, 0, 1, 2)\nX_train_resized = tf.image.resize(X_train, (32, 32))\nX_test_resized = tf.image.resize(X_test, (32, 32))\n\nY_train = tf.keras.utils.to_categorical(Y_train, num_classes=10)\nY_test = tf.keras.utils.to_categorical(Y_test, num_classes=10)\n\n'''\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\n          \nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer='adam',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',   # Metric to monitor (validation loss)\n    patience=5,           # Number of epochs to wait without improvement\n    restore_best_weights=True  # Restore weights from the best epoch\n)\n\nmodel.fit(X_train_resized, Y_train, epochs=20, batch_size=32, callbacks=[early_stopping])\n\nmodel.save('my_model_2.h5')\n'''\n\nmodel = load_model('my_model_2.h5')\n\nloss, accuracy = model.evaluate(X_test, Y_test)\n\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:28:18.774943Z","iopub.execute_input":"2025-01-17T01:28:18.775337Z","iopub.status.idle":"2025-01-17T01:28:33.830369Z","shell.execute_reply.started":"2025-01-17T01:28:18.775303Z","shell.execute_reply":"2025-01-17T01:28:33.829307Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9176 - loss: 0.4413\nTest Loss: 0.4191308915615082\nTest Accuracy: 0.9193684458732605\n","output_type":"stream"}],"execution_count":7}]}