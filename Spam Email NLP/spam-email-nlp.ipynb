{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6230659,"sourceType":"datasetVersion","datasetId":3554536}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T20:58:09.072803Z","iopub.execute_input":"2025-01-21T20:58:09.073290Z","iopub.status.idle":"2025-01-21T20:58:37.925348Z","shell.execute_reply.started":"2025-01-21T20:58:09.073248Z","shell.execute_reply":"2025-01-21T20:58:37.923582Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#Data Preprocessing\n\ndf_train = pd.read_csv(\"/kaggle/input/email-spam-classification/email_spam.csv\")\ndf_train_label = df_train.iloc[:, -1:]\n\nlabel_encoder = LabelEncoder()\ny_train_encoded = df_train_label.apply(lambda col: label_encoder.fit_transform(col) if col.dtypes == 'object' else col)\ndf_train['label'] = y_train_encoded\n\ndf_train['text'] = df_train['title'] + \" \" + df_train['text']\nx_train_data = df_train.text\n\nx_train, x_val, y_train, y_val = train_test_split(\n    df_train['text'], df_train['label'], test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T22:09:08.111221Z","iopub.execute_input":"2025-01-21T22:09:08.111640Z","iopub.status.idle":"2025-01-21T22:09:08.133239Z","shell.execute_reply.started":"2025-01-21T22:09:08.111605Z","shell.execute_reply":"2025-01-21T22:09:08.131885Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n#Tokenizes the entire dataframe\nclass EmailDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n\n# Accuracy calculation function\ndef calculate_accuracy(preds, labels):\n    _, pred_labels = torch.max(preds, dim=1) #First one is the raw data and second is the prediction\n    correct_predictions = (pred_labels == labels).float()\n    accuracy = correct_predictions.sum() / len(correct_predictions)\n    return accuracy\n\n\n\n# Tokenizer and model setup\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Tokenized data\ntrain_dataset = EmailDataset(x_train.to_numpy(), y_train.to_numpy(), tokenizer, max_len=128)\nval_dataset = EmailDataset(x_val.to_numpy(), y_val.to_numpy(), tokenizer, max_len=128)\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=16)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nmodel = model.to(device)\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\ncriterion = torch.nn.CrossEntropyLoss()\n\nnum_epochs = 5\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    total_accuracy = 0\n\n    for batch in train_dataloader:\n        optimizer.zero_grad() #Reset Gradient\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n\n        loss = outputs.loss #Retrieves from loss function and updates weights\n        logits = outputs.logits\n\n        # Calculate accuracy\n        accuracy = calculate_accuracy(logits, labels)\n        total_loss += loss.item()\n        total_accuracy += accuracy.item()\n\n        loss.backward() #Back propagation\n        optimizer.step() #Update Model Parameter\n\n    avg_loss = total_loss / len(train_dataloader)\n    avg_accuracy = total_accuracy / len(train_dataloader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}, Accuracy: {avg_accuracy}\")\n    torch.save(model.state_dict(), 'spam_email_nlp.pth')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-21T22:16:26.166089Z","iopub.execute_input":"2025-01-21T22:16:26.166574Z","iopub.status.idle":"2025-01-21T22:20:02.256640Z","shell.execute_reply.started":"2025-01-21T22:16:26.166538Z","shell.execute_reply":"2025-01-21T22:20:02.255369Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/email-spam-classification/email_spam.csv\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 0.6831975817680359, Accuracy: 0.5666666686534881\nEpoch 2/5, Loss: 0.5862083315849305, Accuracy: 0.7083333373069763\nEpoch 3/5, Loss: 0.437084949016571, Accuracy: 0.8\nEpoch 4/5, Loss: 0.34319655895233153, Accuracy: 0.9\nEpoch 5/5, Loss: 0.24039712250232698, Accuracy: 0.9625\n","output_type":"stream"}],"execution_count":17}]}